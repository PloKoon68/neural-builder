diğer layerlar eklenicek
loss ve activation functionlar eklenicek



notlar:
NN->train() de binary outputa göre lossValue değişkeni atandı


dense layer dakiler genelleştirilmeli diğer layerlarla da (learning rate gibi)


efficiency için layer outputları (türev layerları da) referans olarak ekle 


şu anda layer backward fonksiyonları 


element wise productta 2 boyutlu vector (shape = 1,n) ile matrix çarpımı matrix çarpımı eklenebilir

dense layerlardaki linear işlemleri fonksiyonlaştır kalabalık yapma (mesela activation fonksiyona sokma ve türevini döndürme)


dense layer back propagationdaki türev denkelmelri genelleştirilebilir ayrı fonksiyon şeklinde yazılır, süper class declaration yapılır 

!!bir açık var: backward yaptıktan sonra a ve z aynı kalıyor, peş peşe backward çağırılırsa aynı a ve z yi kullanıp durucak.

neuralnetoworks trainde cost value ilk elemanı ekrana basıyor, softmwax vs genelle.

train overload et tek örnek girilirse: optional

inputların uyumunu kontrol eden checkValidInput fonksiyonu yaz



kaldığım yer: loss m example bölüp tek değer mi döndürmeli sigmoidcrossentropyde


Ahmet ağırakça





a2 = 
0.719207  0.148885
0.957085  0.263625

a3 = 
0,768024914934201  0,5744989857103826

dz3 = -0.231975085065799, 0,5744989857103826


-0,08130382352   -0,07056757918

dw3 = -0.04065191176, -0.03528378959

da2 = 
-0,02439114706	-0,02117027375
-0,03252152941	-0,02822703167

dz2 = 
-0.01512321904816923369383466840556, -0.02070784649350320425706852780368
-0.0145693176425391465519977626657,  -0.02635274213853379562305734414979

dw2 = 
0,01482075467	-0,01982325109	-0,009678860191
0,01427793129	-0,02101820852	-0,009324363291



en son da2 yanlış hesaplanmıştı